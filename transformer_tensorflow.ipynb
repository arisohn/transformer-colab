{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "transformer_tensorflow.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9mHC7Ojk57g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtpVVN97feSh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.core.display import display, HTML\n",
        "display(HTML(\"<style>.container {width:100% !important;} </style>\"))\n",
        "display(HTML(\"<style>.CodeMirror pre {font-family: Monaco; font-size: 9pt;} </style>\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-uSHCcAfeSs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_features      = 50\n",
        "n_timesteps_in  = 5\n",
        "n_timesteps_out = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLuXrBRdfeSx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_layers = 2\n",
        "d_model    = 128\n",
        "dff        = 512\n",
        "num_heads  = 4\n",
        "\n",
        "input_vocab_size  = n_features + 2\n",
        "target_vocab_size = n_features + 2\n",
        "dropout_rate      = 0.1\n",
        "\n",
        "start_token = [n_features]\n",
        "end_token   = [n_features + 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvErISDvfeS1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from random import randint\n",
        " \n",
        "# generate a sequence of random integers\n",
        "def generate_sequence(length, n_unique):\n",
        "  return [randint(0, n_unique-1) for _ in range(length)]\n",
        "\n",
        "# prepare data for the LSTM\n",
        "def get_pair(n_in, n_out, n_unique):\n",
        "  # generate random sequence\n",
        "  temp = generate_sequence(n_in, n_unique)\n",
        "  sequence_in  = start_token + temp + end_token\n",
        "  sequence_out = start_token + temp[:n_out] + end_token\n",
        "  \n",
        "#   # one hot encode\n",
        "#   X = one_hot_encode(sequence_in, n_unique)\n",
        "#   y = one_hot_encode(sequence_out, n_unique)\n",
        "  \n",
        "#   # reshape as 3D\n",
        "#   X = X.reshape((1, X.shape[0], X.shape[1]))\n",
        "#   y = y.reshape((1, y.shape[0], y.shape[1]))\n",
        "  \n",
        "  return sequence_in, sequence_out\n",
        "\n",
        "print(get_pair(5, 2, 50))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ej9WK8EcfeTK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training = []\n",
        "for _ in range(16):\n",
        "  inp_ = []\n",
        "  tar_ = []\n",
        "  for _ in range(512):\n",
        "    inp, tar = get_pair(n_timesteps_in, n_timesteps_out, n_features)\n",
        "    inp_.append(inp)\n",
        "    tar_.append(tar)\n",
        "  training.append((inp_, tar_))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1TuqvTrfeS5",
        "colab_type": "text"
      },
      "source": [
        "### Transformer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gv5o4NeDfeS6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "def get_angles(pos, i, d_model):\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "  return pos * angle_rates\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis], np.arange(d_model)[np.newaxis, :], d_model)\n",
        "  \n",
        "  # apply sin to even indices in the array; 2i\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "  \n",
        "  # apply cos to odd indices in the array; 2i+1\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    \n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "    \n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "  \"\"\"\n",
        "    Calculate the attention weights.\n",
        "    q, k, v must have matching leading dimensions.\n",
        "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "    The mask has different shapes depending on its type(padding or look ahead) \n",
        "    but it must be broadcastable for addition.\n",
        "\n",
        "    Args:\n",
        "      q: query shape == (..., seq_len_q, depth)\n",
        "      k: key shape   == (..., seq_len_k, depth)\n",
        "      v: value shape == (..., seq_len_v, depth_v)\n",
        "      mask: Float tensor with shape broadcastable to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "      output, attention_weights\n",
        "  \"\"\"\n",
        "\n",
        "  matmul_qk = tf.matmul(q, k, transpose_b = True)  # (..., seq_len_q, seq_len_k)\n",
        " \n",
        "  # scale matmul_qk\n",
        "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "  \n",
        "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "  \n",
        "  # add the mask to the scaled tensor.\n",
        "  if mask is not None:\n",
        "    scaled_attention_logits += (mask * -1e9)  \n",
        "\n",
        "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "  # add up to 1.\n",
        "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "  return output, attention_weights\n",
        "\n",
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model   = d_model\n",
        "    \n",
        "    assert d_model % self.num_heads == 0\n",
        "    \n",
        "    self.depth = d_model // self.num_heads\n",
        "    \n",
        "    self.wq = tf.keras.layers.Dense(d_model)\n",
        "    self.wk = tf.keras.layers.Dense(d_model)\n",
        "    self.wv = tf.keras.layers.Dense(d_model)\n",
        "    \n",
        "    self.dense = tf.keras.layers.Dense(d_model)\n",
        "    \n",
        "  def split_heads(self, x, batch_size):\n",
        "    \"\"\"\n",
        "      Split the last dimension into (num_heads, depth).\n",
        "      Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "    \"\"\"\n",
        "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "  \n",
        "  def call(self, v, k, q, mask):\n",
        "    batch_size = tf.shape(q)[0]\n",
        "    \n",
        "    q = self.wq(q) # (batch_size, seq_len, d_model)\n",
        "    k = self.wk(k) # (batch_size, seq_len, d_model)\n",
        "    v = self.wv(v) # (batch_size, seq_len, d_model)\n",
        "    \n",
        "    q = self.split_heads(q, batch_size) # (batch_size, num_heads, seq_len_q, depth)\n",
        "    k = self.split_heads(k, batch_size) # (batch_size, num_heads, seq_len_k, depth)\n",
        "    v = self.split_heads(v, batch_size) # (batch_size, num_heads, seq_len_v, depth)\n",
        "    \n",
        "    # scaled_attention.shape  == (batch_size, num_heads, seq_len_q, depth)\n",
        "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "    \n",
        "    scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
        "    \n",
        "    scaled_attention = tf.transpose(scaled_attention, perm = [0, 2, 1, 3])           # (batch_size, seq_len_q, num_heads, depth)\n",
        "    concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "    \n",
        "    output = self.dense(concat_attention) # (batch_size, seq_len_q, d_model)\n",
        "    \n",
        "    return output, attention_weights\n",
        "\n",
        "\n",
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "  return tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(dff, activation = 'relu'), # (batch_size, seq_len, dff)\n",
        "    tf.keras.layers.Dense(d_model)                   # (batch_size, seq_len, d_model)\n",
        "  ])\n",
        "\n",
        "\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate = 0.1):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "    \n",
        "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "    \n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon = 1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon = 1e-6)\n",
        "    \n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "  def call(self, x, training, mask):\n",
        "    attn_output, attn_weights_block = self.mha(x, x, x, mask)        # (batch_size, input_seq_len, d_model)\n",
        "    attn_output    = self.dropout1(attn_output, training = training)\n",
        "    out1           = self.layernorm1(x + attn_output)                # (batch_size, input_seq_len, d_model)\n",
        "    \n",
        "    ffn_output     = self.ffn(out1)                                  # (batch_size, input_seq_len, d_model)\n",
        "    ffn_output     = self.dropout2(ffn_output, training = training)  \n",
        "    out2           = self.layernorm2(out1 + ffn_output)              # (batch_size, input_seq_len, d_model)\n",
        "    \n",
        "    return out2, attn_weights_block\n",
        "\n",
        "  \n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate = 0.1):\n",
        "    super(Encoder, self).__init__()\n",
        "    \n",
        "    self.d_model    = d_model\n",
        "    self.num_layers = num_layers\n",
        "    \n",
        "    self.embedding    = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
        "    \n",
        "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
        "    self.dropout    = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "  def call(self, x, training, mask):\n",
        "    # (batch, seq, vocab)\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    attention_weights = {}\n",
        "    \n",
        "    # adding embedding and position encoding\n",
        "    x =  self.embedding(x) # (batch_size, input_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, seq_len, :]\n",
        "    \n",
        "    x = self.dropout(x, training = training)\n",
        "    \n",
        "    for i in range(self.num_layers):\n",
        "      x, block = self.enc_layers[i](x, training, mask)\n",
        "      \n",
        "      attention_weights['encoder_layer{}_block'.format(i+1)] = block\n",
        "      \n",
        "    # x.shape = (batch_size, input_seq_len, d_model)\n",
        "    return x, attention_weights\n",
        "  \n",
        "  \n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate = 0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "    \n",
        "    self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "    self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "    \n",
        "    self.ffn  = point_wise_feed_forward_network(d_model, dff)\n",
        "    \n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon = 1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon = 1e-6)\n",
        "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon = 1e-6)\n",
        "    \n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    \n",
        "  def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "    \n",
        "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)                    # (batch_size, target_seq_len, d_model)\n",
        "    attn1                      = self.dropout1(attn1, training = training)\n",
        "    out1                       = self.layernorm1(attn1 + x)\n",
        "    \n",
        "    attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
        "    attn2                      = self.dropout2(attn2, training = training)\n",
        "    out2                       = self.layernorm2(attn2 + out1)                          # (batch_size, target_seq_len, d_model)\n",
        "    \n",
        "    ffn_output                 = self.ffn(out2)                                         # (batch_size, target_seq_len, d_model)\n",
        "    ffn_output                 = self.dropout3(ffn_output, training = training)\n",
        "    out3                       = self.layernorm3(ffn_output + out2)                     # (batch_size, target_seq_len, d_model)\n",
        "    \n",
        "    return out3, attn_weights_block1, attn_weights_block2\n",
        "\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate = 0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "    \n",
        "    self.d_model    = d_model\n",
        "    self.num_layers = num_layers\n",
        "    \n",
        "    self.embedding    = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "    \n",
        "    self.dec_layers   = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
        "    self.dropout      = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "  def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "    seq_len           = tf.shape(x)[1]\n",
        "    attention_weights = {}\n",
        "    \n",
        "    x =  self.embedding(x) # (batch_size, target_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, seq_len, :]\n",
        "    \n",
        "    x = self.dropout(x, training = training)\n",
        "    \n",
        "    for i in range(self.num_layers):\n",
        "      x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
        "    \n",
        "      attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "      attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "      \n",
        "    # x.shape = (batch_size, target_seq_len, d_model)\n",
        "    return x, attention_weights\n",
        "\n",
        "\n",
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate = 0.1):\n",
        "    super(Transformer, self).__init__()\n",
        "    \n",
        "    self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size,  pe_input,  rate)\n",
        "    \n",
        "    self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
        "    \n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "    \n",
        "  def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
        "    # (batch_size, inp_seq_len, d_model)\n",
        "    enc_output, attention_weights_e = self.encoder(inp, training, enc_padding_mask)\n",
        "    \n",
        "    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "    dec_output, attention_weights_d = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "    \n",
        "    # (batch_size, tar_seq_len, target_vocab_size)\n",
        "    final_output = self.final_layer(dec_output)\n",
        "    \n",
        "    return final_output, attention_weights_e, attention_weights_d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foX3ZDv1feS_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_stemps = 4000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "    \n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "    \n",
        "    self.warmup_steps = warmup_stemps\n",
        "    \n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "    \n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "  \n",
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1 = 0.9, beta_2 = 0.98, epsilon = 1e-9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcWaUNQXfeTD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True, reduction = 'none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask  = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "  \n",
        "  mask  =  tf.cast(mask, dtype = loss_.dtype)\n",
        "  loss_ *= mask\n",
        "  \n",
        "  return tf.reduce_mean(loss_)\n",
        "\n",
        "train_loss     = tf.keras.metrics.Mean(name = 'train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name = 'train_accuracy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcCFqjhjq_tV",
        "colab_type": "text"
      },
      "source": [
        "### Learn & Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjLGNuOKfeTP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transformer = Transformer(num_layers, \n",
        "                          d_model, \n",
        "                          num_heads, \n",
        "                          dff, \n",
        "                          input_vocab_size, \n",
        "                          target_vocab_size, \n",
        "                          pe_input = input_vocab_size, \n",
        "                          pe_target = target_vocab_size, \n",
        "                          rate = dropout_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Joy5W7dQfeTS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_look_ahead_mask(size):\n",
        "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "  return mask  # (seq_len, seq_len)\n",
        "\n",
        "\n",
        "def create_padding_mask(seq):\n",
        "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "  \n",
        "  # add extra dimensions to add the padding\n",
        "  # to the attention logits.\n",
        "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
        "\n",
        "\n",
        "def create_masks(inp, tar):\n",
        "  # Encoder padding mask\n",
        "  enc_padding_mask = create_padding_mask(inp)\n",
        "  \n",
        "  # Used in the 2nd attention block in the decoder.\n",
        "  # This padding mask is used to mask the encoder outputs.\n",
        "  dec_padding_mask = create_padding_mask(inp)\n",
        "  \n",
        "  # Used in the 1st attention block in the decoder.\n",
        "  # It is used to pad and mask future tokens in the input received by the decoder.\n",
        "  look_ahead_mask         = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "  dec_target_padding_mask = create_padding_mask(tar)\n",
        "  combined_mask           = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "  \n",
        "  return enc_padding_mask, combined_mask, dec_padding_mask\n",
        "\n",
        "\n",
        "train_step_signature = [\n",
        "  tf.TensorSpec(shape = (None, None), dtype = tf.int64),\n",
        "  tf.TensorSpec(shape = (None, None), dtype = tf.int64),\n",
        "]\n",
        "\n",
        "\n",
        "@tf.function(input_signature = train_step_signature)\n",
        "def train_step(inp, tar):\n",
        "  tar_inp  = tar[:, :-1]\n",
        "  tar_real = tar[:, 1: ]\n",
        "  \n",
        "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "  \n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions, _, _ = transformer(inp, tar_inp, True, enc_padding_mask, combined_mask, dec_padding_mask) # [None, None, 52]\n",
        "\n",
        "    loss = loss_function(tar_real, predictions)\n",
        "    \n",
        "  gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "  \n",
        "  train_loss(loss)\n",
        "  train_accuracy(tar_real, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "Tsqb19LYfeTX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 1024\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  \n",
        "  for (inp, tar) in training:\n",
        "    train_step(inp, tar)\n",
        "    \n",
        "  print('Epochs {} Loss {:.4f} Accuracy {:.4f}'.format(epoch, train_loss.result(), train_accuracy.result()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDBb3wz3feTa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_attention_weights(attention, sentence, result, layer):\n",
        "  print(layer)\n",
        "  \n",
        "  fig = plt.figure(figsize=(16, 8))\n",
        "  \n",
        "  sentence = [*map(lambda x: str(x), sentence)]\n",
        "  \n",
        "  attention = tf.squeeze(attention[layer], axis=0)\n",
        "  \n",
        "  for head in range(attention.shape[0]):\n",
        "    ax = fig.add_subplot(2, 4, head+1)\n",
        "    \n",
        "    # plot the attention weights\n",
        "    im = ax.matshow(attention[head], cmap='viridis')\n",
        "    plt.colorbar(im)\n",
        "    \n",
        "#     ax.matshow(attention[head][:-1, :], cmap='viridis')\n",
        "\n",
        "#     fontdict = {'fontsize': 10}\n",
        "    \n",
        "#     ax.set_xticks(range(len(sentence)+2))\n",
        "#     ax.set_yticks(range(len(result)))\n",
        "    \n",
        "#     ax.set_ylim(len(result)-1.5, -0.5)\n",
        "        \n",
        "#     ax.set_xticklabels(['<start>']+[i for i in sentence]+['<end>'], fontdict=fontdict, rotation=90)\n",
        "#     ax.set_yticklabels([str(i) for i in result],  fontdict=fontdict)\n",
        "    \n",
        "    ax.set_xlabel('Head {}'.format(head+1))\n",
        "  \n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLMuaxIXfeTd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH = 10\n",
        "\n",
        "def evaluate(inp_sentence):\n",
        "  start_token = [n_features]\n",
        "  end_token   = [n_features + 1]\n",
        "\n",
        "  # inp sentence is portuguese, hence adding the start and end token\n",
        "  inp_sentence  = start_token + inp_sentence + end_token\n",
        "  encoder_input = tf.expand_dims(inp_sentence, 0) \n",
        "\n",
        "  # as the target is english, the first word to the transformer should be the english start token.\n",
        "  decoder_input = start_token\n",
        "  output        = tf.expand_dims(decoder_input, 0)\n",
        "\n",
        "  for i in range(MAX_LENGTH):\n",
        "    print(output)\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)\n",
        "\n",
        "    # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "    predictions, attention_weights_e, attention_weights_d = transformer(encoder_input, output, False, enc_padding_mask, combined_mask, dec_padding_mask)\n",
        "    \n",
        "    plot_attention_weights(attention_weights_e, sentence, None, 'encoder_layer1_block')\n",
        "    plot_attention_weights(attention_weights_e, sentence, None, 'encoder_layer2_block')\n",
        "    \n",
        "    plot_attention_weights(attention_weights_d, sentence, None, 'decoder_layer1_block1')\n",
        "    plot_attention_weights(attention_weights_d, sentence, None, 'decoder_layer1_block2')\n",
        "\n",
        "    plot_attention_weights(attention_weights_d, sentence, None, 'decoder_layer2_block1')\n",
        "    plot_attention_weights(attention_weights_d, sentence, None, 'decoder_layer2_block2')\n",
        "    \n",
        "    # select the last word from the seq_len dimension\n",
        "    predictions = predictions[:, -1, :]  # (batch_size, 1, vocab_size)\n",
        "\n",
        "    print(predictions)\n",
        "\n",
        "    predicted_id = tf.cast(tf.argmax(predictions, axis = -1), tf.int32)\n",
        "\n",
        "    # return the result if the predicted_id is equal to the end token\n",
        "    if predicted_id == end_token:\n",
        "      return tf.squeeze(output, axis = 0), attention_weights_e , attention_weights_d\n",
        "    \n",
        "    predicted_id = tf.expand_dims(predicted_id, 0)\n",
        "    \n",
        "    # concatentate the predicted_id to the output which is given to the decoder as its input.\n",
        "    output = tf.concat([output, predicted_id], axis = -1)\n",
        "    \n",
        "  return tf.squeeze(output, axis = 0), attention_weights_e, attention_weights_d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "iYkcqCZ7feTh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence = [12, 33, 20, 27, 20]\n",
        "_, _, _ = evaluate(sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1m9GLAyloGOy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}